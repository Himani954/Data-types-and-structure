{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHrvREf7Nsqnohf/kDMPjQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himani954/Data-types-and-structure/blob/main/Decision_Tree_%7C_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is a Decision Tree, and how does it work in the context of classification?**"
      ],
      "metadata": {
        "id": "eOYKrJT8Eo-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans1.**\n",
        "# Decision Tree Overview\n",
        "A Decision Tree is a type of supervised learning algorithm used for both classification and regression tasks. In the context of classification, a Decision Tree is a tree-like model of decisions that leads to a class label.\n",
        "\n",
        "How Decision Trees Work for Classification\n",
        "1. Tree Structure : A Decision Tree consists of nodes (decision points) and edges (outcomes of decisions). The top node is the root node. Each internal node represents a feature or attribute to split the data. Each leaf node represents a class label.\n",
        "\n",
        "2. Splitting Data : At each internal node, the algorithm decides how to split the data based on a feature and a threshold. The goal is to split the data into subsets that are more \"pure\" in terms of class labels.\n",
        "\n",
        "3. Decision Criteria : Common criteria for deciding splits include Gini impurity and information gain (based on entropy).\n",
        "    - Gini Impurity : Measures the impurity of a node. A lower Gini impurity indicates a more pure node.\n",
        "    - Information Gain : Measures the reduction in entropy (or increase in purity) after a split.\n",
        "\n",
        "4. Stopping Criteria : The tree growing stops when a stopping criterion is met, such as when all instances in a node belong to the same class, when a maximum depth is reached, or when the number of instances in a node is below a threshold.\n",
        "\n",
        "5. Prediction : To classify a new instance, you start at the root node and follow the tree based on the feature values of the instance until you reach a leaf node. The class label of the leaf node is the predicted class.\n",
        "\n",
        "Example of Decision Tree in Classification\n",
        "Consider a classification problem to predict whether a person is likely to buy a computer based on age and income. A Decision Tree might split first on age (e.g., <=30 vs. >30), then on income for one of those branches. The final leaf nodes would give the predicted class (buy or not buy).\n",
        "\n",
        "Advantages and Considerations\n",
        "- Advantages : Decision Trees are easy to interpret and visualize. They can handle both numerical and categorical data.\n",
        "- Considerations : Decision Trees can overfit if not pruned or if too deep. They are sensitive to the data used for training."
      ],
      "metadata": {
        "id": "cgXBCIl8E3Nb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n"
      ],
      "metadata": {
        "id": "j-UewtfGGsN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans2.**\n",
        "# Gini Impurity and Entropy as Impurity Measures\n",
        "In Decision Trees, both Gini impurity and Entropy are used as criteria to decide how to split the data at each node. They measure the \"impurity\" or \"disorder\" of a node in terms of the class labels.\n",
        "\n",
        "Gini Impurity\n",
        "- Definition : Gini impurity for a node is calculated as \\(Gini = 1 - \\sum_{i=1}^{C} p_i^2\\), where \\(p_i\\) is the proportion of class \\(i\\) in the node, and \\(C\\) is the number of classes.\n",
        "- *Interpretation*: Gini impurity ranges from 0 (pure node, all instances belong to one class) to \\(1 - \\frac{1}{C}\\) (for a node with equal distribution among \\(C\\) classes).\n",
        "- Use in Decision Trees : The algorithm chooses splits to minimize Gini impurity, leading to more pure child nodes.\n",
        "\n",
        "Entropy\n",
        "- Definition : Entropy for a node is calculated as \\(Entropy = - \\sum_{i=1}^{C} p_i \\log_2(p_i)\\), where \\(p_i\\) is the proportion of class \\(i\\).\n",
        "- *Interpretation*: Entropy measures the uncertainty or disorder in the node. Lower entropy means more purity.\n",
        "- Use in Decision Trees : Splits are chosen to maximize information gain, which is the reduction in entropy from the parent node to the child nodes.\n",
        "\n",
        "Impact on Splits in a Decision Tree\n",
        "- Both Gini impurity and Entropy guide the splitting process by evaluating the quality of a split.\n",
        "- The goal is to create child nodes that are purer (lower Gini impurity or lower Entropy) than the parent node.\n",
        "- Difference in Practice : While both lead to similar trees in many cases, Gini impurity is computationally simpler and is the default in some implementations like scikit-learn. Entropy (via information gain) is more commonly associated with the ID3 and C4.5 algorithms.\n",
        "\n",
        "Example\n",
        "Consider a node with classes A (40%), B (30%), C (30%).\n",
        "- Gini impurity = \\(1 - (0.4^2 + 0.3^2 + 0.3^2)\\).\n",
        "- Entropy = \\(-0.4\\log_2(0.4) - 0.3\\log_2(0.3) - 0.3\\log_2(0.3)\\).\n",
        "\n",
        "Both metrics would guide the Decision Tree to split this node in a way that increases purity in the child nodes.\n"
      ],
      "metadata": {
        "id": "Lzmgha4YG7OF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**"
      ],
      "metadata": {
        "id": "gTRLm4RaH_rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans3.**\n",
        "# Pre-Pruning vs. Post-Pruning in Decision Trees\n",
        "- Pre-Pruning (Early Stopping) : Stops the growth of the Decision Tree early based on certain criteria like maximum depth, minimum number of samples to split a node, or minimum number of samples in a leaf. The tree is not fully grown.\n",
        "- Post-Pruning : Grows the Decision Tree to its maximum size, then prunes back some branches to reduce overfitting. Pruning is based on criteria like reduced error pruning or cost complexity pruning.\n",
        "\n",
        "Practical Advantages\n",
        "1. Pre-Pruning :\n",
        "    - Advantage : Can be computationally more efficient as it stops growing the tree early, avoiding unnecessary computations for branches that wouldn't contribute much.\n",
        "2. Post-Pruning :\n",
        "    - Advantage : Can lead to a more optimal tree since the full tree is grown and then pruned, allowing for a more informed decision on which branches to prune based on the complete tree structure."
      ],
      "metadata": {
        "id": "sBRV5H2OIJ8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**"
      ],
      "metadata": {
        "id": "C35I_9oOJJHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans4.**\n",
        "# Information Gain in Decision Trees\n",
        "- Definition : Information Gain is a measure of the reduction in entropy (or uncertainty) in the target variable due to a split based on a particular feature. It's calculated as \\(Information\\ Gain = Entropy(parent) - \\sum \\frac{N_{child}}{N_{parent}} Entropy(child)\\).\n",
        "- Purpose in Decision Trees : Information Gain is used to decide which feature to use for splitting at a node. The feature with the highest Information Gain is chosen because it reduces uncertainty the most.\n",
        "\n",
        "Importance for Choosing the Best Split\n",
        "- Reducing Uncertainty : By maximizing Information Gain, the Decision Tree algorithm chooses splits that most effectively reduce uncertainty about the class labels, leading to more pure child nodes.\n",
        "- Feature Selection at Each Node : At each node, calculating Information Gain for each feature helps decide which feature best splits the data for classification.\n",
        "\n",
        "Example\n",
        "If a split based on a feature reduces entropy significantly (high Information Gain), it means that split is very effective in separating the classes."
      ],
      "metadata": {
        "id": "JTX8ng6YJQmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "\n",
        "# **Dataset Info:**\n",
        "# **● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).**\n",
        "\n",
        "# **● Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV).**"
      ],
      "metadata": {
        "id": "jpdeIfZcKGu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans5.**\n",
        "# Common Real-World Applications of Decision Trees\n",
        "1. Classification Tasks : Decision Trees are used in areas like credit scoring (predicting loan defaults), medical diagnosis (classifying diseases based on symptoms and tests), and customer segmentation.\n",
        "2. Regression Tasks : They can predict continuous outcomes like housing prices (as in the Boston Housing Dataset) or stock prices.\n",
        "\n",
        "Main Advantage\n",
        "- Interpretability : Decision Trees are easy to interpret and visualize. The tree structure allows for straightforward understanding of how decisions are made based on feature values.\n",
        "\n",
        "Main Limitations\n",
        "- Overfitting : Decision Trees can overfit the training data if not pruned or if too deep, leading to poor generalization on unseen data.\n",
        "- Instability : Small changes in the data can lead to a very different tree being generated.\n",
        "\n",
        "Example with Given Datasets\n",
        "- Iris Dataset (Classification) : A Decision Tree can classify iris flowers into one of three species based on features like sepal length, sepal width, petal length, and petal width.\n",
        "- Boston Housing Dataset (Regression) : A Decision Tree can predict housing prices based on features like crime rate, number of rooms, etc."
      ],
      "metadata": {
        "id": "HTs2uC9Si8bJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6: Write a Python program to:**\n",
        "# **● Load the Iris Dataset**\n",
        "# **● Train a Decision Tree Classifier using the Gini criterion**\n",
        "# **● Print the model’s accuracy and feature importances.**"
      ],
      "metadata": {
        "id": "_1TTWxmVjE0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7hOMjq1jm_v",
        "outputId": "1f513c25-f80d-4721-93be-d6436479fc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Write a Python program to:**\n",
        "# **● Load the Iris Dataset**\n",
        "# **● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.**\n"
      ],
      "metadata": {
        "id": "hKirele2kMv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model 1: Decision Tree with max_depth=3\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "# Model 2: Fully grown Decision Tree (no max_depth)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracy comparison\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_depth3:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWiwHEHOkcYD",
        "outputId": "b598fe0a-c7be-4dc7-b5a0-4a0f31341f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy with fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: Write a Python program to:**\n",
        "# **● Load the California Housing dataset from sklearn**\n",
        "# **● Train a Decision Tree Regressor**\n",
        "# **● Print the Mean Squared Error (MSE) and feature importances**"
      ],
      "metadata": {
        "id": "vAk2lnonlawe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate Mean Squared Error\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the Mean Squared Error\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(data.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_wIMcxalq_l",
        "outputId": "008bfd24-6d85-4170-fd59-79c0ceeae0d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.50\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Write a Python program to:**\n",
        "# **● Load the Iris Dataset**\n",
        "\n",
        "# **● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV**\n",
        "\n",
        "# **● Print the best parameters and the resulting model accuracy**\n"
      ],
      "metadata": {
        "id": "imVelTKrmLtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 4, 6, 8]\n",
        "}\n",
        "\n",
        "# Create the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best estimator and predict on test data\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clOxsDlWmjIO",
        "outputId": "9a93c221-f5dc-406c-e3e5-f8a07516e9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.**\n",
        "\n",
        "# **Explain the step-by-step process you would follow to:**\n",
        "\n",
        "# **● Handle the missing values**\n",
        "\n",
        "# **● Encode the categorical features**\n",
        "\n",
        "# **● Train a Decision Tree model**\n",
        "\n",
        "# **● Tune its hyperparameters**\n",
        "\n",
        "# **● Evaluate its performance**\n",
        "\n",
        "# **And describe what business value this model could provide in the real-world setting.**"
      ],
      "metadata": {
        "id": "mxcdtm5Fm2Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans10.**\n",
        "As a Data Scientist at a healthcare company, your goal is to predict whether a patient has a certain disease using a large dataset that includes both numerical and categorical features, and contains missing values.\n",
        "1. Handling Missing Values\n",
        "\n",
        "Identify missing data:\n",
        "\n",
        "Use .isnull().sum() to understand where missing values exist.\n",
        "\n",
        "Imputation strategies:\n",
        "\n",
        "For numerical features: Impute using the mean or median depending on distribution.\n",
        "\n",
        "For categorical features: Impute using the mode (most frequent value).\n",
        "\n",
        "Consider more advanced imputation (e.g., KNN imputer or IterativeImputer) if the dataset is complex.\n",
        "\n",
        "2. Encoding Categorical Features\n",
        "\n",
        "Use One-Hot Encoding for low-cardinality categorical variables (e.g., gender, region).\n",
        "\n",
        "Use Ordinal Encoding if the feature has a logical order (e.g., severity: low, medium, high).\n",
        "\n",
        "For high-cardinality variables, consider Target Encoding or Frequency Encoding.\n",
        "\n",
        "Ensure encoding is applied consistently on both training and test sets.\n",
        "\n",
        "3. Train a Decision Tree Model\n",
        "\n",
        "Split the data into training and testing sets using train_test_split() (typically 70/30 or 80/20).\n",
        "\n",
        "Train the model:\n"
      ],
      "metadata": {
        "id": "Ad0oUnQ7s8Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "G0Fv9ju2tVaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Tune Hyperparameters\n",
        "\n",
        "Use GridSearchCV or RandomizedSearchCV to find the best combination of hyperparameters.\n",
        "\n",
        "Important parameters to tune:\n",
        "\n",
        "max_depth: Limits tree depth to avoid overfitting.\n",
        "\n",
        "min_samples_split: Minimum samples required to split a node.\n",
        "\n",
        "min_samples_leaf: Minimum samples required at a leaf node.\n",
        "\n",
        "criterion: Either \"gini\" or \"entropy\".\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "NEnj-OljtYaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 15],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "u3ZI5FBJtdg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Evaluate Performance\n",
        "\n",
        "Use relevant classification metrics:\n",
        "\n",
        "Accuracy: Overall correct predictions.\n",
        "\n",
        "Precision and Recall: Crucial in medical settings (e.g., false positives vs false negatives).\n",
        "\n",
        "F1-Score: Balances precision and recall.\n",
        "\n",
        "ROC-AUC Score: Measures separability between classes.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "poLvSPzytgI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1]))"
      ],
      "metadata": {
        "id": "BJyhclTjtlZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Value in Real-World Setting\n",
        "Early Detection: Helps identify at-risk patients early for timely intervention.\n",
        "\n",
        "Improved Outcomes: Enables doctors to tailor treatment plans, improving patient health.\n",
        "\n",
        "Resource Optimization: Focuses tests and treatments on the right patients, reducing waste.\n",
        "\n",
        "Cost Savings: Avoids unnecessary diagnostic procedures, reducing expenses for both patients and the hospital.\n",
        "\n",
        "Compliance and Reporting: Helps healthcare providers meet regulatory standards and provide better data transparency."
      ],
      "metadata": {
        "id": "iBdIhRVhtpfx"
      }
    }
  ]
}