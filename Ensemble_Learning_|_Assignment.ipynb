{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxviLV4+WzyRmkHanCFX26",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himani954/Data-types-and-structure/blob/main/Ensemble_Learning_%7C_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n"
      ],
      "metadata": {
        "id": "EiENc1OKRKFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer 1:**\n",
        "# Ensemble Learning Overview\n",
        "- Definition : Ensemble Learning is a technique in machine learning where multiple models (called \"weak learners\") are combined to create a stronger model (the \"ensemble\") that performs better than any individual model.\n",
        "- Key Idea : The main idea behind ensemble methods is that by combining the predictions of several models, you can often get better performance than using a single model. This leverages the strengths of different models or different configurations of the same model type.\n",
        "\n",
        "Common Ensemble Techniques\n",
        "-  Bagging : Bootstrap aggregating (like in Random Forests) reduces variance.\n",
        "- Boosting : Sequentially adding models to correct  of previous models (like AdaBoost, Gradient Boosting) reduces bias."
      ],
      "metadata": {
        "id": "u0pDfasaRX1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: What is the difference between Bagging and Boosting?**"
      ],
      "metadata": {
        "id": "XJzi_HfrSxlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer 2:**\n",
        "# Bagging vs. Boosting\n",
        "- Bagging :\n",
        "    - Trains models in parallel on different data subsets.\n",
        "    - Reduces variance.\n",
        "    - Example: Random Forest.\n",
        "\n",
        "- Boosting :\n",
        "    - Trains models sequentially, focusing on previous errors.\n",
        "    - Reduces bias.\n",
        "    - Examples: AdaBoost, Gradient Boosting."
      ],
      "metadata": {
        "id": "S6_sjL1LS5Jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**"
      ],
      "metadata": {
        "id": "C1AZ7W72WUx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer 3:**\n",
        "# Bootstrap Sampling Overview\n",
        "- Definition : Bootstrap sampling is a technique where you create subsets of data by sampling with replacement from the original dataset. Each subset is the same size as the original dataset.\n",
        "- Result : Some data points may be repeated in a subset, while others might be left out (out-of-bag data).\n",
        "\n",
        "Role in Bagging (like Random Forest)\n",
        "- In Bagging : Each tree in a Random Forest is trained on a bootstrap sample of the data.\n",
        "- Benefits :\n",
        "    - Reduces overfitting by averaging predictions across trees trained on different samples.\n",
        "    - Allows estimation of out-of-bag error for validation.\n"
      ],
      "metadata": {
        "id": "PJhZBw-mWg-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**"
      ],
      "metadata": {
        "id": "ZCqqj7xqXTJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer 4:**\n",
        "# Out-of-Bag (OOB) Samples\n",
        "- Definition : In Bagging methods like Random Forest, for each tree, OOB samples are the data points that were not included in the bootstrap sample used to train that tree.\n",
        "- Usage : OOB samples can be used to estimate the model's performance without needing a separate validation set.\n",
        "\n",
        "OOB Score for Evaluation\n",
        "- OOB Score : For each data point, predictions are made using trees where that point was OOB. The OOB score is the aggregate of these predictions for evaluating model performance.\n",
        "- Usefulness : Provides an unbiased estimate of model performance using training data itself."
      ],
      "metadata": {
        "id": "ecZ2GIUFXzPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**"
      ],
      "metadata": {
        "id": "CayKTYUuZQZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer 5:**\n",
        "# Feature Importance Analysis: Decision Tree vs. Random Forest\n",
        "-  Single Decision Tree :\n",
        "    - Feature importance can be calculated based on how much each feature contributes to reducing impurity (like Gini impurity).\n",
        "    - Can be less reliable due to overfitting or variance in a single tree.\n",
        "\n",
        "- Random Forest :\n",
        "    - Feature importance is often calculated by averaging the importance across all trees.\n",
        "    - More robust and reliable due to averaging over many trees, reducing variance.\n",
        "\n",
        "Key Difference\n",
        "- Stability : Random Forest feature importances are generally more stable and trustworthy because they're based on an ensemble of trees."
      ],
      "metadata": {
        "id": "Xj6w-j_qZfca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6: Write a Python program to:**\n",
        "# **● Load the Breast Cancer dataset using**\n",
        "# **sklearn.datasets.load_breast_cancer()**\n",
        "# **● Train a Random Forest Classifier**\n",
        "# **● Print the top 5 most important features based on feature importance scores.**"
      ],
      "metadata": {
        "id": "uTVU8h4Sa-Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Create and train the Random Forest Classifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzSpnbQgbaMB",
        "outputId": "2e97e38c-70b1-4d2e-d683-60b072e287e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Write a Python program to:**\n",
        "# **● Train a Bagging Classifier using Decision Trees on the Iris dataset**\n",
        "# **● Evaluate its accuracy and compare with a single Decision Tree**"
      ],
      "metadata": {
        "id": "3L3se-NQb084"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn\n",
        "import sys\n",
        "\n",
        "# Print versions for debugging\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# 4. Train a Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(), # Changed from base_estimator to estimator\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# 5. Print results\n",
        "print(\"Accuracy of Single Decision Tree: {:.4f}\".format(acc_dt))\n",
        "print(\"Accuracy of Bagging Classifier : {:.4f}\".format(acc_bag))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH1D1dpWsD4A",
        "outputId": "0765e8bc-87a6-4a97-eedc-75c7be70140f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "Scikit-learn version: 1.6.1\n",
            "Accuracy of Single Decision Tree: 0.9333\n",
            "Accuracy of Bagging Classifier : 0.9333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: Write a Python program to:**\n",
        "# **● Train a Random Forest Classifier**\n",
        "# **● Tune hyperparameters max_depth and n_estimators using GridSearchCV**\n",
        "# **● Print the best parameters and final accuracy**"
      ],
      "metadata": {
        "id": "8svpl3ALsLMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate on test data\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Final Accuracy on Test Set:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zN0Hjd7OsdxX",
        "outputId": "dcd9571d-e818-492f-c0a3-3c1dd4c91f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Accuracy on Test Set: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Write a Python program to:**\n",
        "# **● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset**\n",
        "# **● Compare their Mean Squared Errors (MSE)**"
      ],
      "metadata": {
        "id": "JIWbwjvRs48s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Bagging Regressor (using DecisionTree as base estimator)\n",
        "bagging = BaggingRegressor(n_estimators=100, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Calculate MSE\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bag)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bagging)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)\n",
        "\n",
        "# Compare\n",
        "if mse_bagging < mse_rf:\n",
        "    print(\"Bagging Regressor performed better.\")\n",
        "else:\n",
        "    print(\"Random Forest Regressor performed better.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynqE33UvtHPK",
        "outputId": "f0acf193-7a15-4988-be00-76b79bafa237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25592438609899626\n",
            "Mean Squared Error (Random Forest Regressor): 0.2553684927247781\n",
            "Random Forest Regressor performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.**\n",
        "# **You decide to use ensemble techniques to increase model performance.**\n",
        "# **Explain your step-by-step approach to:**\n",
        "# **● Choose between Bagging or Boosting**\n",
        "# **● Handle overfitting**\n",
        "# **● Select base models**\n",
        "# **● Evaluate performance using cross-validation**\n",
        "# **● Justify how ensemble learning improves decision-making in this real-world context.**\n"
      ],
      "metadata": {
        "id": "wHwSOg_Fw2Y2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer10:**\n",
        "Step-by-Step Approach for Predicting Loan Default Using Ensemble Techniques\n",
        "● Choose between Bagging or Boosting\n",
        "- For predicting loan default, *Boosting* might be more suitable because it sequentially corrects errors, focusing on harder-to-classify instances (like customers on the edge of defaulting). Boosting techniques like Gradient Boosting or XGBoost are effective in handling complex patterns in financial data and reducing bias.\n",
        "\n",
        "● Handle Overfitting\n",
        "- Regularization : Use regularization techniques available in boosting algorithms like XGBoost ( gamma , lambda , alpha) to prevent overfitting.\n",
        "- Early Stopping : Implement early stopping based on a validation set to stop training when performance stops improving.\n",
        "- Cross-Validation : Use cross-validation to assess how the model generalizes to unseen data.\n",
        "\n",
        "● Select Base Models\n",
        "- For Boosting: Decision trees are commonly used as base learners because they are weak learners that Boosting can improve upon.\n",
        "- Choose shallow trees (like max_depth=3-5 ) to keep individual trees weak but let Boosting build a strong ensemble.\n",
        "\n",
        "● Evaluate Performance using Cross-Validation\n",
        "- Use k-fold cross-validation to get a robust estimate of model performance on unseen data.\n",
        "- Evaluate using metrics relevant for loan default prediction like AUC-ROC , precision , recall , or F1-score , considering class imbalance (since defaults are typically rare).\n",
        "\n",
        "● Justify How Ensemble Learning Improves Decision-Making\n",
        "- Improved Accuracy : Ensemble methods like Boosting often lead to higher accuracy by combining multiple models.\n",
        "- Handling Complex Relationships : Ensemble techniques are effective in capturing complex interactions between customer demographics and transaction history.\n",
        "- Better Risk Assessment : In loan default prediction, improved accuracy means better risk assessment, leading to smarter lending decisions, reduced losses, and optimized portfolio management.\n",
        "\n",
        "Summary\n",
        "In predicting loan default using ensemble techniques:\n",
        "- Boosting is chosen for its ability to reduce bias.\n",
        "- Overfitting is handled via regularization and early stopping.\n",
        "- Weak learners like shallow decision trees are used.\n",
        "- Cross-validation evaluates performance robustly.\n",
        "- Ensemble learning improves decision-making by enhancing prediction accuracy for better risk management."
      ],
      "metadata": {
        "id": "nESEPe0Hxhwz"
      }
    }
  ]
}