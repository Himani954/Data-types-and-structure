{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhFnNRyZ791aI7zH2rUpwg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himani954/Data-types-and-structure/blob/main/KNN_%26_PCA_%7C_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**"
      ],
      "metadata": {
        "id": "ypLHLVEF8LdF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer1:**\n",
        "# K-Nearest Neighbors (KNN)\n",
        "K-Nearest Neighbors (KNN) is a simple, non-parametric, and lazy learning algorithm used for both classification and regression tasks.\n",
        "\n",
        "How KNN Works\n",
        "1. Distance Calculation : KNN calculates the distance between the input data point and all other points in the training dataset. Common distance metrics include Euclidean, Manhattan, or Minkowski distance.\n",
        "2. Finding Nearest Neighbors : It identifies the K closest data points (neighbors) to the input point based on the calculated distances.\n",
        "3. Prediction :\n",
        "    - Classification : The class of the input data point is determined by a majority vote of its K neighbors.\n",
        "    - Regression : The prediction is the average of the target values of the K nearest neighbors.\n",
        "\n",
        "Characteristics of KNN\n",
        "- Lazy Learning : KNN doesn't build an explicit model during training; it defers computation until prediction time.\n",
        "- Non-Parametric : KNN makes no assumptions about the underlying data distribution.\n",
        "- Sensitive to K and Distance Metric : Performance depends on choosing the right value of K and the distance metric.\n",
        "\n",
        "Use Cases\n",
        "- Classification : KNN is used in tasks like handwriting recognition or pattern recognition.\n",
        "- Regression : KNN can predict continuous outcomes based on neighbor averages."
      ],
      "metadata": {
        "id": "xtSFbYdN8T1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?**"
      ],
      "metadata": {
        "id": "1U8SFRsQ9t4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer2 :**\n",
        "# The Curse of Dimensionality\n",
        "The curse of dimensionality refers to the problems that arise when dealing with data in high-dimensional spaces. As the number of features (dimensions) increases, data becomes increasingly sparse, and distances between points become less meaningful.\n",
        "\n",
        "How It Affects KNN Performance\n",
        "1. Distance Becomes Less Meaningful : In high dimensions, distances between points tend to become similar, making it harder for KNN to find meaningful nearest neighbors.\n",
        "2. Increased Computational Cost : Computing distances in high-dimensional spaces is more expensive.\n",
        "3. Data Sparsity : High-dimensional data is often sparse, making it difficult for KNN to generalize well due to lack of nearby points.\n",
        "\n",
        "Mitigating the Curse of Dimensionality for KNN\n",
        "- Dimensionality Reduction : Techniques like PCA or t-SNE can reduce dimensions while preserving important information.\n",
        "- Feature Selection : Selecting relevant features reduces dimensionality and can improve KNN performance."
      ],
      "metadata": {
        "id": "yyvVEA7R923G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3: What is Principal Component Analysis (PCA)? How is it different from feature selection?**"
      ],
      "metadata": {
        "id": "skKC_Svm-3pi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer3 :**\n",
        "# Principal Component Analysis (PCA)\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining as much variance (information) as possible.\n",
        "\n",
        "How PCA Works\n",
        "1. Computes Covariance Matrix : PCA calculates the covariance matrix of the data to understand feature relationships.\n",
        "2. Finds Eigenvectors and Eigenvalues : It computes eigenvectors (principal components) and eigenvalues of the covariance matrix. Eigenvalues indicate the amount of variance explained by each principal component.\n",
        "3. Projects Data : Data is projected onto the top principal components to reduce dimensions.\n",
        "\n",
        "Difference from Feature Selection\n",
        "- Feature Selection : Selects a subset of the original feature based on criteria like relevance or importance.\n",
        "- PCA : Creates new features (principal components) that are linear combinations of the original features, aiming to capture maximum variance.\n",
        "\n",
        "Use Cases for PCA\n",
        "- Dimensionality Reduction : PCA reduces dimensions while retaining data structure.\n",
        "- Noise Reduction : By focusing on components with high variance, PCA can reduce noise."
      ],
      "metadata": {
        "id": "_V-FD-7A_C9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?**\n"
      ],
      "metadata": {
        "id": "w9j1YASsAJBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer 4:**\n",
        "# Eigenvalues and Eigenvectors in PCA\n",
        "In Principal Component Analysis (PCA), eigenvalues and eigenvectors are key concepts derived from the covariance matrix of the data.\n",
        "\n",
        "Importance of Eigenvalues and Eigenvectors\n",
        "- Eigenvectors (Principal Components) : Eigenvectors represent the directions of maximum variance in the data. These are the principal components onto which the data is projected for dimensionality reduction.\n",
        "- Eigenvalues : Eigenvalues indicate the amount of variance explained by each corresponding eigenvector (principal component). Larger eigenvalues mean more variance is captured by that component.\n",
        "\n",
        "Why They Are Important\n",
        "- Dimensionality Reduction : By sorting eigenvalues in descending order, you can choose the top principal components that capture most of the data's variance, reducing dimensions effectively.\n",
        "- Explained Variance : Eigenvalues help determine how much of the data's variability is explained by each principal component."
      ],
      "metadata": {
        "id": "s3wIRpW8AQrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: How do KNN and PCA complement each other when applied in a single pipeline?**"
      ],
      "metadata": {
        "id": "Hbw05YV0BLdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer5 :**\n",
        "# KNN and PCA in a Single Pipeline\n",
        "KNN (K-Nearest Neighbors) and PCA (Principal Component Analysis) can complement each other in a machine learning pipeline.\n",
        "\n",
        "How They Complement Each Other\n",
        "- PCA for Dimensionality Reduction : Applying PCA before KNN reduces the dimensionality of the data, addressing the curse of dimensionality that affects KNN in high dimensions.\n",
        "- KNN for Classification/Regression Post-PCA : After PCA reduces dimensions, KNN can perform classification or regression in the lower-dimensional space more effectively.\n",
        "\n",
        "Benefits of Combining PCA and KNN\n",
        "- Improved KNN Performance : PCA helps by reducing dimensions, making distance calculations in KNN more meaningful.\n",
        "- Reduced Computational Cost : Lower dimensions from PCA reduce computational costs for KNN.\n",
        "\n",
        "Typical Pipeline\n",
        "1. Apply PCA to reduce data dimensions.\n",
        "2. Use KNN for classification or regression in the reduced space."
      ],
      "metadata": {
        "id": "lWGdSJiMBTxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.**"
      ],
      "metadata": {
        "id": "tisGfVNCCCmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# ---------------- Without Scaling ----------------\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# ---------------- With Scaling ----------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Results\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_with_scaling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vJ91KbOCUD6",
        "outputId": "08aa854d-1fac-4c1b-8922-259ca3f05149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.**"
      ],
      "metadata": {
        "id": "MpDrS9HRCm9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features before PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Print explained variance ratio\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPISlbrFCt0r",
        "outputId": "04727bf4-d504-473b-cdc8-5c8b4a3d5b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.**"
      ],
      "metadata": {
        "id": "ExRP-p1rC4JB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---------------- Original Dataset ----------------\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "accuracy_original = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# ---------------- PCA with 2 Components ----------------\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(\n",
        "    X_pca, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train_pca)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test_pca, y_pred_pca)\n",
        "\n",
        "# Results\n",
        "print(\"KNN Accuracy on Original Dataset:\", accuracy_original)\n",
        "print(\"KNN Accuracy on PCA (2 components):\", accuracy_pca)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRty_yhRC_ZQ",
        "outputId": "6c97b20a-f92a-45c0-f877-7c7cde92f864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy on Original Dataset: 0.9444444444444444\n",
            "KNN Accuracy on PCA (2 components): 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.**"
      ],
      "metadata": {
        "id": "ieiZ8RupDL1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ---------------- Euclidean Distance ----------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# ---------------- Manhattan Distance ----------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Results\n",
        "print(\"KNN Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "print(\"KNN Accuracy with Manhattan distance:\", accuracy_manhattan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j__kAXXlDbKK",
        "outputId": "a7f3ad69-5f9c-4125-a198-7b4c7a4163fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy with Euclidean distance: 0.9444444444444444\n",
            "KNN Accuracy with Manhattan distance: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10: You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.**\n",
        "# **Due to the large number of features and a small number of samples, traditional models overfit.**\n",
        "\n",
        "# **Explain how you would:**\n",
        "# **● Use PCA to reduce dimensionality**\n",
        "\n",
        "# **● Decide how many components to keep**\n",
        "# **● Use KNN for classification post-dimensionality reduction**\n",
        "# **● Evaluate the model**\n",
        "# **● Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data**\n"
      ],
      "metadata": {
        "id": "GLPUJW_JD8eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer10 :**\n",
        "\n",
        "# Approach for Classifying Cancer Types Using PCA and KNN\n",
        "● Use PCA to Reduce Dimensionality\n",
        "- Apply PCA to the high-dimensional gene expression dataset to reduce dimensions. PCA transforms data into principal components capturing maximum variance.\n",
        "\n",
        "● Decide How Many Components to Keep\n",
        "- Use a scree plot or choose components explaining a cumulative variance threshold (e.g., 90-95%). This balances dimensionality reduction with retaining sufficient information.\n",
        "\n",
        "● Use KNN for Classification Post-Dimensionality Reduction\n",
        "- After reducing dimensions with PCA, apply KNN for classifying cancer types in the lower-dimensional space. KNN works better in reduced dimensions.\n",
        "\n",
        "● Evaluate the Model\n",
        "- Use cross-validation to assess model performance. Metrics like accuracy , F1-score, or AUC-ROC evaluate classification performance, considering class imbalance in cancer types.\n",
        "\n",
        "● Justify Pipeline to Stakeholders\n",
        "- Handling High Dimensionality : PCA reduces dimensions, addressing overfitting in traditional models with many features and few samples.\n",
        "- Robust for Biomedical Data : PCA + KNN is effective for high-dimensional genomic data where feature selection is challenging. Cross-validation ensures generalizability.\n",
        "- Interpretability and Performance : This pipeline balances complexity reduction with retaining enough variance for accurate cancer type classification.\n",
        "\n",
        "Summary\n",
        "Using PCA for dimensionality reduction followed by KNN classification is a robust approach for high-dimensional gene expression data with few samples. Cross-validation ensures model evaluation is reliable."
      ],
      "metadata": {
        "id": "6SKIVvQ-Fv0I"
      }
    }
  ]
}