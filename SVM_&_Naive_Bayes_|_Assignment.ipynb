{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlvhjNOrGVRFwTj+gFJxQi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Himani954/Data-types-and-structure/blob/main/SVM_%26_Naive_Bayes_%7C_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is a Support Vector Machine (SVM), and how does it work?**"
      ],
      "metadata": {
        "id": "ri4ThfrFcvvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans1.**\n",
        "# Support Vector Machine (SVM) Overview\n",
        "- Definition : A Support Vector Machine is a supervised learning algorithm used for classification and regression tasks. SVMs are particularly known for their effectiveness in high-dimensional spaces.\n",
        "- Goal : In classification, SVM finds the hyperplane that best separates the classes in the feature space.\n",
        "\n",
        "How SVM Works\n",
        "1. Finding the Hyperplane : SVM aims to find the hyperplane that maximally separates the classes. In a two-dimensional space, this is a line; in higher dimensions, it's a hyperplane.\n",
        "2. Support Vectors : The data points closest to the hyperplane are called support vectors. These points are crucial in defining the hyperplane.\n",
        "3. Margin Maximization : SVM maximizes the margin (distance) between the hyperplane and the support vectors of each class.\n",
        "4. Kernel Trick : For non-linearly separable data, SVM uses kernel functions to transform the data into a higher-dimensional space where a hyperplane can separate the classes.\n",
        "\n",
        "Key Aspects of SVM\n",
        "- Kernel Choice : Common kernels include linear, polynomial, radial basis function (RBF). The choice of kernel affects the SVM's ability to capture the data's structure.\n",
        "- Regularization Parameter (C) : Controls the trade-off between achieving a low-error solution and a solution that generalizes well for new data.\n",
        "\n",
        "Example\n",
        "In a binary classification problem with two features, SVM would find a line (or hyperplane in higher dimensions) that best separates the two classes with the maximum margin."
      ],
      "metadata": {
        "id": "TDtq69pdc4r0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: Explain the difference between Hard Margin and Soft Margin SVM.**"
      ],
      "metadata": {
        "id": "VETToS-KeRvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans2.**\n",
        "# Hard Margin vs. Soft Margin SVM\n",
        "- Hard Margin SVM :\n",
        "    - Definition : A Hard Margin SVM is used when the data is linearly separable. It aims to find the hyperplane that separates the classes with the maximum margin, without allowing any data points to be on the wrong side of the margin.\n",
        "    - Issue : Hard Margin SVMs can be sensitive to outliers and won't work well if the data isn't perfectly linearly separable.\n",
        "\n",
        "- Soft Margin SVM :\n",
        "    - Definition : A Soft Margin SVM allows for some misclassifications (data points on the wrong side of the margin) by introducing slack variables. This makes SVM more robust to noise and outliers.\n",
        "    - Controlled by C : The regularization parameter \\(C\\) controls the trade-off between maximizing the margin and minimizing the classification error. A smaller \\(C\\) allows for a wider margin (softer margin), potentially at the cost of more misclassifications.\n",
        "\n",
        "Key Difference\n",
        "- Flexibility : Soft Margin SVM is more flexible and commonly used because real-world data is often not perfectly separable.\n",
        "- Usage : Soft Margin (via introducing slack variables and using \\(C\\)) is more practical for most applications."
      ],
      "metadata": {
        "id": "iMRJ73dWe3Ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.**\n"
      ],
      "metadata": {
        "id": "J4lvubr0f5Xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans3.**\n",
        "# Kernel Trick in SVM\n",
        "- Definition : The kernel trick in SVM allows the algorithm to operate in a higher-dimensional feature space without explicitly transforming the data into that space. This enables SVM to find a separating hyperplane in a space where the data might be linearly separable, even if it's not in the original space.\n",
        "- How it works : By using a kernel function, SVM computes the inner product of two vectors in the higher-dimensional space without actually mapping the vectors to that space, thus saving computational resources.\n",
        "\n",
        "Example of a Kernel: Radial Basis Function (RBF)\n",
        "- RBF Kernel : \\(K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2)\\)\n",
        "- Use Case : The RBF kernel is widely used for non-linear classification problems. It's effective when the relationship between features is complex and non-linear. \\(\\gamma\\) controls the influence of each support vector."
      ],
      "metadata": {
        "id": "tXSaEbXMgOvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?**"
      ],
      "metadata": {
        "id": "oyj9sla3hRl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans4.**\n",
        "# Naive Bayes Classifier Overview\n",
        "- Definition : A Naive Bayes classifier is a probabilistic machine learning model based on Bayes' theorem. It's used for classification tasks.\n",
        "- How it works : Naive Bayes calculates the probability of each class given the input features and predicts the class with the highest probability.\n",
        "\n",
        "Why is it Called \"Naive\"?\n",
        "- Assumption of Independence : The \"naive\" part comes from the assumption that all features are independent of each other given the class. This is often not true in real-world data but simplifies calculations and often works surprisingly well in practice.\n",
        "\n",
        "Characteristics of Naive Bayes\n",
        "- Simple and Fast : Naive Bayes is easy to implement and computationally efficient.\n",
        "- Performs Well with High-Dimensional Data : Despite its simplicity and \"naive\" assumption, it can perform well, especially with a large number of features.\n"
      ],
      "metadata": {
        "id": "7vVIC308hbyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?**"
      ],
      "metadata": {
        "id": "0ajIRPwwicVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans5.**\n",
        "\n",
        "# Gaussian, Multinomial, and Bernoulli Naive Bayes Variants\n",
        "1. Gaussian Naive Bayes\n",
        "- Assumption : Features are continuous and follow a Gaussian distribution for each class.\n",
        "- Use Case : Suitable for datasets with continuous features like the Iris dataset (`sklearn.datasets.load_iris()`).\n",
        "\n",
        "2. Multinomial Naive Bayes\n",
        "- Assumption : Features represent counts (like word counts in a document).\n",
        "- Use Case : Commonly used in text classification tasks.\n",
        "\n",
        "3. Bernoulli Naive Bayes\n",
        "- Assumption : Features are binary (like presence/absence of a word).\n",
        "- Use Case : Used when features are binary, like in some text classification scenarios focusing on presence/absence.\n",
        "\n",
        "Example with Datasets\n",
        "- Iris with Gaussian NB : Gaussian Naive Bayes can be applied to the Iris dataset for classification based on sepal and petal measurements.\n",
        "- Text data with Multinomial/Bernoulli NB : For text classification tasks, Multinomial (for word counts) or Bernoulli (for binary word presence) Naive Bayes can be used."
      ],
      "metadata": {
        "id": "WgmIzHx3jlq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6: Write a Python program to:**\n",
        "# **● Load the Iris dataset**\n",
        "# **● Train an SVM Classifier with a linear kernel**\n",
        "# **● Print the model's accuracy and support vectors.**\n"
      ],
      "metadata": {
        "id": "oJR5YEr6lSV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM classifier with linear kernel\n",
        "svm_clf = SVC(kernel='linear')\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print support vectors\n",
        "print(\"Support Vectors:\")\n",
        "print(svm_clf.support_vectors_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKmkKy9KlwtA",
        "outputId": "dc9ff66a-47be-42f3-d3e8-6d2a4b3a5f00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Write a Python program to:**\n",
        "# **● Load the Breast Cancer dataset**\n",
        "# **● Train a Gaussian Naïve Bayes model**\n",
        "# **● Print its classification report including precision, recall, and F1-score.**"
      ],
      "metadata": {
        "id": "GcJMzkndmqs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71W3MSzKnAR_",
        "outputId": "c92db125-3d57-47f8-bf81-6265fa165bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: Write a Python program to:**\n",
        "# **● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.**\n",
        "# **● Print the best hyperparameters and accuracy.**"
      ],
      "metadata": {
        "id": "ytCyX_1MnSLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Create SVM model and perform GridSearchCV\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set with best model\n",
        "y_pred = grid.predict(X_test)\n",
        "\n",
        "# Print best hyperparameters and accuracy\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHZNQSAFnqwG",
        "outputId": "ad329268-253a-43d4-ab06-326756e3afd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Model Accuracy: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: Write a Python program to:**\n",
        "\n",
        "# **● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using**\n",
        "# **sklearn.datasets.fetch_20newsgroups).**\n",
        "\n",
        "# **● Print the model's ROC-AUC score for its predictions.**\n"
      ],
      "metadata": {
        "id": "VEQ6A9UToDGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load a subset of the 20 Newsgroups dataset for binary classification\n",
        "categories = ['sci.space', 'rec.sport.hockey']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(data.data)\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_proba = nb.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32fq9voIoU1v",
        "outputId": "8b9928dd-1150-4eff-8920-4af2ce6c21a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10: Imagine you’re working as a data scientist for a company that handles email communications.**\n",
        "\n",
        "# **Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:**\n",
        "\n",
        "# **● Text with diverse vocabulary**\n",
        "\n",
        "# **● Potential class imbalance (far more legitimate emails than spam)**\n",
        "\n",
        "# **● Some incomplete or missing data**\n",
        "# **Explain the approach you would take to:**\n",
        "\n",
        "# **● Preprocess the data (e.g. text vectorization, handling missing data)**\n",
        "\n",
        "# **● Choose and justify an appropriate model (SVM vs. Naïve Bayes)**\n",
        "\n",
        "# **● Address class imbalance**\n",
        "\n",
        "# **● Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution.**"
      ],
      "metadata": {
        "id": "ZWV3q8z4ozGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ans10.**\n",
        "# Approach to Classifying Emails as Spam or Not Spam\n",
        "Preprocess the Data\n",
        "- Text Vectorization : Use techniques like Bag-of-Words or TF-IDF to convert text into numerical vectors. TF-IDF is often more effective for spam classification.\n",
        "- Handling Missing Data : If there are missing values in features other than text (unlikely in text-focused spam detection), use imputation like filling with the most frequent value or using a model-based imputation.\n",
        "\n",
        "Choose and Justify an Appropriate Model\n",
        "- Naïve Bayes vs. SVM :\n",
        "    - Naïve Bayes : Often a good choice for text classification due to its simplicity and efficiency with high-dimensional sparse data like text.\n",
        "    - SVM : Can be effective but might be computationally more expensive with large vocabularies.\n",
        "- Choice : Naïve Bayes (Multinomial) is often preferred for text classification tasks like spam detection due to its efficiency and effectiveness.\n",
        "\n",
        "Address Class Imbalance\n",
        "- Class Imbalance Handling : Use techniques like SMOTE, class weights in the model (like `class_weight='balanced'` in sklearn classifiers), or adjust the decision threshold to handle imbalance.\n",
        "\n",
        "Evaluate Performance with Suitable Metrics\n",
        "- Metrics : Use precision, recall, F1-score (especially for the minority class), and ROC AUC or PR AUC for imbalanced data. Accuracy alone can be misleading with class imbalance.\n",
        "\n",
        "Business Impact of the Solution\n",
        "- Reduced Manual Effort : Automatically classifying spam reduces the need for manual filtering, saving time.\n",
        "- lmproved User Experience : More accurate spam filtering means less spam in inboxes and fewer false positives losing important emails.\n",
        "- Security : Effective spam detection can reduce phishing and malicious email risks.\n",
        "\n",
        "Example Implementation"
      ],
      "metadata": {
        "id": "tMWdHSwPrA7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming emails_df has 'text' and 'label' (0=Not Spam, 1=Spam)\n",
        "X_train, X_test, y_train, y_test = train_test_split(emails_df['text'], emails_df['label'], test_size=0.2)\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "nb = MultinomialNB(class_prior=[0.9, 0.1]) # Adjusting for imbalance\n",
        "nb.fit(X_train_vec, y_train)\n",
        "y_pred = nb.predict(X_test_vec)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "F0cXR_vFr5qx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}